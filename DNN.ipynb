{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DNN.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN6xF+1K1Icf2KVhgXsY1CH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lplsz/Machine_Learning/blob/main/DNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nUUpbYZUSpMh"
      },
      "source": [
        "**Notation**:\n",
        "- Superscript $[l]$ denotes a quantity associated with the $l^{th}$ layer. \n",
        "    - Example: $a^{[L]}$ is the $L^{th}$ layer activation. $W^{[L]}$ and $b^{[L]}$ are the $L^{th}$ layer parameters.\n",
        "- Superscript $(i)$ denotes a quantity associated with the $i^{th}$ example. \n",
        "    - Example: $x^{(i)}$ is the $i^{th}$ training example.\n",
        "- Lowerscript $i$ denotes the $i^{th}$ entry of a vector.\n",
        "    - Example: $a^{[l]}_i$ denotes the $i^{th}$ entry of the $l^{th}$ layer's activations).\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**General Methodology**\n",
        "As usual you will follow the Deep Learning methodology to build the model:\n",
        "1. Initialize parameters / Define hyperparameters\n",
        "    2. Loop for num_iterations:\n",
        "        a. Forward propagation\n",
        "        b. Compute cost function\n",
        "        c. Backward propagation\n",
        "        d. Update parameters (using parameters, and grads from backprop) \n",
        "    4. Use trained parameters to predict labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZEHz4_qJgR1T"
      },
      "source": [
        "**Preprosseing Dataset**\n",
        "\n",
        "\n",
        "*   Flatten image data\n",
        "*   Standardize\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dwgW89TVdKe6"
      },
      "source": [
        "train_x_orig, train_y, test_x_orig, test_y, classes = load_data()\n",
        "\n",
        "m_train = train_x_orig.shape[0]     # Size of training set\n",
        "num_px = train_x_orig.shape[1]      # Height and width of an training image\n",
        "m_test = test_x_orig.shape[0]       # Size of testing set\n",
        "\n",
        "# Reshape the training and test example into a rank 1 vector\n",
        "# Shape before (num_px, num_px, 3)\n",
        "# Shape now: (num_px  ∗∗  num_px  ∗∗  3, 1).\n",
        "train_x_flatten = train_x_orig.reshape(train_x_orig.shape[0], -1).T   # The \"-1\" makes reshape flatten the remaining dimensions\n",
        "test_x_flatten = test_x_orig.reshape(test_x_orig.shape[0], -1).T\n",
        "\n",
        "# Standardize data to have feature values between 0 and 1.\n",
        "train_x = train_x_flatten/255.\n",
        "test_x = test_x_flatten/255."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fuep9hNBTUbt"
      },
      "source": [
        "**Initizalization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3uPWiOAfSdSx"
      },
      "source": [
        "# 第l层中w的大小为n_l * n_(l-1)，b为n_l * 1\n",
        "\n",
        "def initialize_parameters_deep(layer_dims):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "    layer_dims -- python array (list) containing the dimensions of each layer in our network\n",
        "    \n",
        "    Returns:\n",
        "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
        "                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
        "                    bl -- bias vector of shape (layer_dims[l], 1)\n",
        "    \"\"\"\n",
        "    np.random.seed(1)\n",
        "    parameters = {}\n",
        "    L = len(layer_dims)\n",
        "\n",
        "    for l in range(1, L):\n",
        "        # Random initialization\n",
        "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) / np.sqrt(layer_dims[l-1]) \n",
        "        # All zeros\n",
        "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1)) \n",
        "        \n",
        "    return parameters"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0c9mvfUYRoh"
      },
      "source": [
        "**Forward Propagation**\n",
        "$$Z^{[l]} = W^{[l]}A^{[l-1]} +b^{[l]}$$\n",
        "\n",
        "where $A^{[0]} = X$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IYLk6OzwSddi"
      },
      "source": [
        "def sigmoid(Z): # sigmoid函数\n",
        "    A = 1/(1+np.exp(-Z))\n",
        "    cache = Z\n",
        "    \n",
        "    return A, cache\n",
        "\n",
        "def relu(Z): # relu函数\n",
        "    A = np.maximum(0, Z)\n",
        "    cache = Z \n",
        "    \n",
        "    return A, cache\n",
        "\n",
        "def linear_activation_forward(A_prev, W, b, activation): # A single layer\n",
        "    Z = np.dot(W, A_prev) + b\n",
        "    linear_cache = (A_prev, W, b)\n",
        "    \n",
        "    if activation == \"sigmoid\": \n",
        "        A, activation_cache = sigmoid(Z)\n",
        "    elif activation == \"relu\":\n",
        "        A, activation_cache = relu(Z)\n",
        "    \n",
        "    # Linear_cache: A_[l-1], W_[l], b_[l]\n",
        "    # Activation_cache: Z = WA + b\n",
        "    cache = (linear_cache, activation_cache)\n",
        "\n",
        "    return A, cache\n",
        "\n",
        "def L_model_forward(X, parameters): # L Layers\n",
        "    caches = []   # Cache the intermedia computations\n",
        "    A = X\n",
        "    L = len(parameters) // 2                 \n",
        "\n",
        "    for l in range(1, L):\n",
        "        A_prev = A \n",
        "        A, cache = linear_activation_forward(A_prev, parameters['W' + str(l)], parameters['b' + str(l)], activation = \"relu\")\n",
        "        caches.append(cache)\n",
        "    \n",
        "    AL, cache = linear_activation_forward(A, parameters['W' + str(L)], parameters['b' + str(L)], activation = \"sigmoid\")\n",
        "    caches.append(cache)\n",
        "    \n",
        "    return AL, caches\n",
        "\n",
        "def compute_cost(AL, Y): # 计算成本\n",
        "    m = Y.shape[1]\n",
        "\n",
        "    cost = (1./m) * (-np.dot(Y,np.log(AL).T) - np.dot(1-Y, np.log(1-AL).T)) # 成本函数\n",
        "    cost = np.squeeze(cost)\n",
        "\n",
        "    return cost"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxHah_R-ZANk"
      },
      "source": [
        "**Back Propagation**\n",
        "$$ dW^{[l]} = \\frac{\\partial \\mathcal{J} }{\\partial W^{[l]}} = \\frac{1}{m} dZ^{[l]} A^{[l-1] T} \\tag{8}$$\n",
        "$$ db^{[l]} = \\frac{\\partial \\mathcal{J} }{\\partial b^{[l]}} = \\frac{1}{m} \\sum_{i = 1}^{m} dZ^{[l](i)}\\tag{9}$$\n",
        "$$ dA^{[l-1]} = \\frac{\\partial \\mathcal{L} }{\\partial A^{[l-1]}} = W^{[l] T} dZ^{[l]} \\tag{10}$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FnoteGudSdmS"
      },
      "source": [
        "# Backward Propagation\n",
        "def sigmoid_backward(dA, cache): # sigmoid单元求导\n",
        "    Z = cache\n",
        "    s = 1/(1 + np.exp(-Z))\n",
        "    dZ = dA * s * (1-s)\n",
        "    \n",
        "    return dZ\n",
        "\n",
        "def relu_backward(dA, cache): # relu单元求导\n",
        "    Z = cache\n",
        "    dZ = np.array(dA, copy=True)\n",
        "    dZ[Z <= 0] = 0\n",
        "    \n",
        "    return dZ\n",
        "\n",
        "def linear_activation_backward(dA, cache, activation): # Single Layer\n",
        "    '''\n",
        "      Input: dA_[l]\n",
        "      Output: dA_[l-1], dW_[l], db_[l]\n",
        "    '''\n",
        "\n",
        "    linear_cache, activation_cache = cache\n",
        "    \n",
        "    if activation == \"relu\":\n",
        "        dZ = relu_backward(dA, activation_cache)\n",
        "    elif activation == \"sigmoid\":\n",
        "        dZ = sigmoid_backward(dA, activation_cache)\n",
        "    \n",
        "    A_prev, W, b = linear_cache\n",
        "    m = A_prev.shape[1]\n",
        "\n",
        "    dW = np.dot(dZ, A_prev.T)/m\n",
        "    db = np.sum(dZ, axis=1, keepdims=True)/m\n",
        "    dA_prev = np.dot(W.T, dZ)\n",
        "    \n",
        "    return dA_prev, dW, db\n",
        "\n",
        "def L_model_backward(AL, Y, caches): # L Layers: the whole back-propagation\n",
        "    grads = {}\n",
        "    L = len(caches)\n",
        "    m = AL.shape[1]\n",
        "    Y = Y.reshape(AL.shape) \n",
        "    \n",
        "    dAL = -(np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
        "    current_cache = caches[L-1]\n",
        "    grads[\"dA\" + str(L)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, current_cache, activation = \"sigmoid\")\n",
        "    \n",
        "    for l in reversed(range(L-1)):\n",
        "        current_cache = caches[l]\n",
        "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l + 2)], current_cache, activation = \"relu\")\n",
        "        grads[\"dA\" + str(l + 1)] = dA_prev_temp\n",
        "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
        "        grads[\"db\" + str(l + 1)] = db_temp\n",
        "\n",
        "    return grads\n",
        "\n",
        "def update_parameters(parameters, grads, learning_rate): # 更新参数\n",
        "    L = len(parameters) // 2 \n",
        "\n",
        "    for l in range(L): \n",
        "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate*grads[\"dW\" + str(l+1)]\n",
        "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate*grads[\"db\" + str(l+1)]\n",
        "\n",
        "    return parameters"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bri-2OaOaKlP"
      },
      "source": [
        "**The DNN Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vSY_AZGNSdtV"
      },
      "source": [
        "# 计算准确度\n",
        "\n",
        "def predict(X, y, parameters):  # One pass through the network\n",
        "    m = X.shape[1]\n",
        "    n = len(parameters) // 2 \n",
        "    p = np.zeros((1,m))\n",
        "    \n",
        "    probas, caches = L_model_forward(X, parameters)\n",
        "\n",
        "    for i in range(0, probas.shape[1]):\n",
        "        if probas[0,i] > 0.5:\n",
        "            p[0,i] = 1\n",
        "        else:\n",
        "            p[0,i] = 0\n",
        "    \n",
        "    print(\"预测准确度: \"  + str(np.sum((p == y)/m)))\n",
        "        \n",
        "    return p\n",
        "  \n",
        "# 训练L层神经网络\n",
        "\n",
        "def L_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):\n",
        "\n",
        "    np.random.seed(1)\n",
        "    costs = []                  \n",
        "    \n",
        "    parameters = initialize_parameters_deep(layers_dims)\n",
        "\n",
        "    # Each iteration is trained using the whole training set via vectorisation\n",
        "    for i in range(0, num_iterations):\n",
        "        AL, caches = L_model_forward(X, parameters)\n",
        "        cost = compute_cost(AL, Y)\n",
        "        grads = L_model_backward(AL, Y, caches)\n",
        "        parameters = update_parameters(parameters, grads, learning_rate)\n",
        "                \n",
        "        if print_cost and i % 100 == 0:\n",
        "            print (\"循环%i次后的成本值: %f\" %(i, cost))\n",
        "        if print_cost and i % 100 == 0:\n",
        "            costs.append(cost)\n",
        "            \n",
        "    plt.plot(np.squeeze(costs))\n",
        "    plt.ylabel('cost')\n",
        "    plt.xlabel('iterations (per tens)')\n",
        "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
        "    plt.show()\n",
        "    \n",
        "    return parameters"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}